{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (__init__.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/junxie/anaconda/lib/python3.6/site-packages/forecaster/__init__.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def c\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from dataframe import DataFrame\n",
    "from datareader import DataReader\n",
    "from tf_basemodel import TFBaseModel\n",
    "from tf_utils import (\n",
    "    time_distributed_dense_layer, temporal_convolution_layer,\n",
    "    sequence_mean, sequence_smape, shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_paths = [\n",
    "    \"/data/kaggle-wikipedia/data2/\",\n",
    "    \"/Users/jiayou/Dropbox/JuanCode/Insight/Project/tf-data/\",\n",
    "    \"/Users/jiayou/Dropbox/Documents/JuanCode/Kaggle/Wikipedia/data2/\",\n",
    "    '/mnt/WD Black/Dropbox/JuanCode/Insight/Project/tf-data/',\n",
    "    '/home/paperspace/Documents/insight/insight/tf-data/',\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cnn(TFBaseModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        residual_channels=32,\n",
    "        skip_channels=32,\n",
    "        dilations=[2**i for i in range(8)]*3,\n",
    "        filter_widths=[2 for i in range(8)]*3,\n",
    "        num_decode_steps=64,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.residual_channels = residual_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.num_decode_steps = num_decode_steps\n",
    "        super(cnn, self).__init__(**kwargs)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return tf.log(x + 1) - tf.expand_dims(self.log_x_encode_mean, 1)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return tf.exp(x + tf.expand_dims(self.log_x_encode_mean, 1)) - 1\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.x_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.encode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.y_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "        self.decode_len = tf.placeholder(tf.int32, [None])\n",
    "        self.is_nan_encode = tf.placeholder(tf.float32, [None, None])\n",
    "        self.is_nan_decode = tf.placeholder(tf.float32, [None, self.num_decode_steps])\n",
    "\n",
    "        self.page_id = tf.placeholder(tf.int32, [None])\n",
    "        self.project = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        self.log_x_encode_mean = sequence_mean(tf.log(self.x_encode + 1), self.encode_len)\n",
    "        self.log_x_encode = self.transform(self.x_encode)\n",
    "        self.x = tf.expand_dims(self.log_x_encode, 2)\n",
    "\n",
    "        self.encode_features = tf.concat([\n",
    "            tf.expand_dims(self.is_nan_encode, 2),\n",
    "            tf.expand_dims(tf.cast(tf.equal(self.x_encode, 0.0), tf.float32), 2),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, tf.shape(self.x_encode)[1], 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        decode_idx = tf.tile(tf.expand_dims(tf.range(self.num_decode_steps), 0), (tf.shape(self.y_decode)[0], 1))\n",
    "        self.decode_features = tf.concat([\n",
    "            tf.one_hot(decode_idx, self.num_decode_steps),\n",
    "            tf.tile(tf.reshape(self.log_x_encode_mean, (-1, 1, 1)), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.project, 9), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.access, 3), 1), (1, self.num_decode_steps, 1)),\n",
    "            tf.tile(tf.expand_dims(tf.one_hot(self.agent, 2), 1), (1, self.num_decode_steps, 1)),\n",
    "        ], axis=2)\n",
    "\n",
    "        return self.x\n",
    "\n",
    "    def encode(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-encode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-encode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-encode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-encode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-encode-2')\n",
    "\n",
    "        return y_hat, conv_inputs[:-1]\n",
    "\n",
    "    def initialize_decode_params(self, x, features):\n",
    "        x = tf.concat([x, features], axis=2)\n",
    "\n",
    "        inputs = time_distributed_dense_layer(\n",
    "            inputs=x,\n",
    "            output_units=self.residual_channels,\n",
    "            activation=tf.nn.tanh,\n",
    "            scope='x-proj-decode'\n",
    "        )\n",
    "\n",
    "        skip_outputs = []\n",
    "        conv_inputs = [inputs]\n",
    "        for i, (dilation, filter_width) in enumerate(zip(self.dilations, self.filter_widths)):\n",
    "            dilated_conv = temporal_convolution_layer(\n",
    "                inputs=inputs,\n",
    "                output_units=2*self.residual_channels,\n",
    "                convolution_width=filter_width,\n",
    "                causal=True,\n",
    "                dilation_rate=[dilation],\n",
    "                scope='dilated-conv-decode-{}'.format(i)\n",
    "            )\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "            outputs = time_distributed_dense_layer(\n",
    "                inputs=dilated_conv,\n",
    "                output_units=self.skip_channels + self.residual_channels,\n",
    "                scope='dilated-conv-proj-decode-{}'.format(i)\n",
    "            )\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "\n",
    "            inputs += residuals\n",
    "            conv_inputs.append(inputs)\n",
    "            skip_outputs.append(skips)\n",
    "\n",
    "        skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=2))\n",
    "        h = time_distributed_dense_layer(skip_outputs, 128, scope='dense-decode-1', activation=tf.nn.relu)\n",
    "        y_hat = time_distributed_dense_layer(h, 1, scope='dense-decode-2')\n",
    "        return y_hat\n",
    "\n",
    "    def decode(self, x, conv_inputs, features):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # initialize state tensor arrays\n",
    "        state_queues = []\n",
    "        for i, (conv_input, dilation) in enumerate(zip(conv_inputs, self.dilations)):\n",
    "            batch_idx = tf.range(batch_size)\n",
    "            batch_idx = tf.tile(tf.expand_dims(batch_idx, 1), (1, dilation))\n",
    "            batch_idx = tf.reshape(batch_idx, [-1])\n",
    "\n",
    "            queue_begin_time = self.encode_len - dilation - 1\n",
    "            temporal_idx = tf.expand_dims(queue_begin_time, 1) + tf.expand_dims(tf.range(dilation), 0)\n",
    "            temporal_idx = tf.reshape(temporal_idx, [-1])\n",
    "\n",
    "            idx = tf.stack([batch_idx, temporal_idx], axis=1)\n",
    "            slices = tf.reshape(tf.gather_nd(conv_input, idx), (batch_size, dilation, shape(conv_input, 2)))\n",
    "\n",
    "            layer_ta = tf.TensorArray(dtype=tf.float32, size=dilation + self.num_decode_steps)\n",
    "            layer_ta = layer_ta.unstack(tf.transpose(slices, (1, 0, 2)))\n",
    "            state_queues.append(layer_ta)\n",
    "\n",
    "        # initialize feature tensor array\n",
    "        features_ta = tf.TensorArray(dtype=tf.float32, size=self.num_decode_steps)\n",
    "        features_ta = features_ta.unstack(tf.transpose(features, (1, 0, 2)))\n",
    "\n",
    "        # initialize output tensor array\n",
    "        emit_ta = tf.TensorArray(size=self.num_decode_steps, dtype=tf.float32)\n",
    "\n",
    "        # initialize other loop vars\n",
    "        elements_finished = 0 >= self.decode_len\n",
    "        time = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        # get initial x input\n",
    "        current_idx = tf.stack([tf.range(tf.shape(self.encode_len)[0]), self.encode_len - 1], axis=1)\n",
    "        initial_input = tf.gather_nd(x, current_idx)\n",
    "\n",
    "        def loop_fn(time, current_input, queues):\n",
    "            current_features = features_ta.read(time)\n",
    "            current_input = tf.concat([current_input, current_features], axis=1)\n",
    "\n",
    "            with tf.variable_scope('x-proj-decode', reuse=True):\n",
    "                w_x_proj = tf.get_variable('weights')\n",
    "                b_x_proj = tf.get_variable('biases')\n",
    "                x_proj = tf.nn.tanh(tf.matmul(current_input, w_x_proj) + b_x_proj)\n",
    "\n",
    "            skip_outputs, updated_queues = [], []\n",
    "            for i, (conv_input, queue, dilation) in enumerate(zip(conv_inputs, queues, self.dilations)):\n",
    "\n",
    "                state = queue.read(time)\n",
    "                with tf.variable_scope('dilated-conv-decode-{}'.format(i), reuse=True):\n",
    "                    w_conv = tf.get_variable('weights'.format(i))\n",
    "                    b_conv = tf.get_variable('biases'.format(i))\n",
    "                    dilated_conv = tf.matmul(state, w_conv[0, :, :]) + tf.matmul(x_proj, w_conv[1, :, :]) + b_conv\n",
    "                conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=1)\n",
    "                dilated_conv = tf.nn.tanh(conv_filter)*tf.nn.sigmoid(conv_gate)\n",
    "\n",
    "                with tf.variable_scope('dilated-conv-proj-decode-{}'.format(i), reuse=True):\n",
    "                    w_proj = tf.get_variable('weights'.format(i))\n",
    "                    b_proj = tf.get_variable('biases'.format(i))\n",
    "                    concat_outputs = tf.matmul(dilated_conv, w_proj) + b_proj\n",
    "                skips, residuals = tf.split(concat_outputs, [self.skip_channels, self.residual_channels], axis=1)\n",
    "\n",
    "                x_proj += residuals\n",
    "                skip_outputs.append(skips)\n",
    "                updated_queues.append(queue.write(time + dilation, x_proj))\n",
    "\n",
    "            skip_outputs = tf.nn.relu(tf.concat(skip_outputs, axis=1))\n",
    "            with tf.variable_scope('dense-decode-1', reuse=True):\n",
    "                w_h = tf.get_variable('weights')\n",
    "                b_h = tf.get_variable('biases')\n",
    "                h = tf.nn.relu(tf.matmul(skip_outputs, w_h) + b_h)\n",
    "\n",
    "            with tf.variable_scope('dense-decode-2', reuse=True):\n",
    "                w_y = tf.get_variable('weights')\n",
    "                b_y = tf.get_variable('biases')\n",
    "                y_hat = tf.matmul(h, w_y) + b_y\n",
    "\n",
    "            elements_finished = (time >= self.decode_len)\n",
    "            finished = tf.reduce_all(elements_finished)\n",
    "\n",
    "            next_input = tf.cond(\n",
    "                finished,\n",
    "                lambda: tf.zeros([batch_size, 1], dtype=tf.float32),\n",
    "                lambda: y_hat\n",
    "            )\n",
    "            next_elements_finished = (time >= self.decode_len - 1)\n",
    "\n",
    "            return (next_elements_finished, next_input, updated_queues)\n",
    "\n",
    "        def condition(unused_time, elements_finished, *_):\n",
    "            return tf.logical_not(tf.reduce_all(elements_finished))\n",
    "\n",
    "        def body(time, elements_finished, emit_ta, *state_queues):\n",
    "            (next_finished, emit_output, state_queues) = loop_fn(time, initial_input, state_queues)\n",
    "\n",
    "            emit = tf.where(elements_finished, tf.zeros_like(emit_output), emit_output)\n",
    "            emit_ta = emit_ta.write(time, emit)\n",
    "\n",
    "            elements_finished = tf.logical_or(elements_finished, next_finished)\n",
    "            return [time + 1, elements_finished, emit_ta] + list(state_queues)\n",
    "\n",
    "        returned = tf.while_loop(\n",
    "            cond=condition,\n",
    "            body=body,\n",
    "            loop_vars=[time, elements_finished, emit_ta] + state_queues\n",
    "        )\n",
    "\n",
    "        outputs_ta = returned[2]\n",
    "        y_hat = tf.transpose(outputs_ta.stack(), (1, 0, 2))\n",
    "        return y_hat\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "\n",
    "        y_hat_encode, conv_inputs = self.encode(x, features=self.encode_features)\n",
    "        self.initialize_decode_params(x, features=self.decode_features)\n",
    "        y_hat_decode = self.decode(y_hat_encode, conv_inputs, features=self.decode_features)\n",
    "        y_hat_decode = self.inverse_transform(tf.squeeze(y_hat_decode, 2))\n",
    "        y_hat_decode = tf.nn.relu(y_hat_decode)\n",
    "\n",
    "        self.labels = self.y_decode\n",
    "        self.preds = y_hat_decode\n",
    "        self.loss = sequence_smape(self.labels, self.preds, self.decode_len, self.is_nan_decode)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'priors': self.x_encode,\n",
    "            'labels': self.labels,\n",
    "            'preds': self.preds,\n",
    "            'page_id': self.page_id,\n",
    "        }\n",
    "\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nn(reader):\n",
    "    return cnn(\n",
    "        reader=reader,\n",
    "        log_dir=os.path.join('./tf-data', 'logs'),\n",
    "        checkpoint_dir=os.path.join('./tf-data', 'checkpoints'),\n",
    "        prediction_dir=os.path.join('./tf-data', 'predictions'),\n",
    "        optimizer='adam',\n",
    "        learning_rate=.001,\n",
    "        batch_size=128,\n",
    "        num_training_steps=20000,#200000\n",
    "        early_stopping_steps=500,#5000\n",
    "        warm_start_init_step=0,\n",
    "        regularization_constant=0.0,\n",
    "        keep_prob=1.0,\n",
    "        enable_parameter_averaging=False,\n",
    "        num_restarts=2,\n",
    "        min_steps_to_checkpoint=100,#500\n",
    "        log_interval=10,\n",
    "        num_validation_batches=1,\n",
    "        grad_clip=20,\n",
    "        residual_channels=32,\n",
    "        skip_channels=32,\n",
    "        dilations=[2**i for i in range(8)]*3,\n",
    "        filter_widths=[2 for i in range(8)]*3,\n",
    "        num_decode_steps=64,\n",
    "    )\n",
    "\n",
    "reader = DataReader(\n",
    "    data_dir=os.path.join(root, 'processed/')\n",
    ")\n",
    "\n",
    "nn = get_nn(reader)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('training start at:', datetime.now().strftime('%Y-%m-%d.%H-%M-%S.%f'))\n",
    "nn.fit()\n",
    "print('training ends at:', datetime.now().strftime('%Y-%m-%d.%H-%M-%S.%f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = DataReader(\n",
    "    data_dir=os.path.join(root, 'processed_full/')\n",
    ")\n",
    "\n",
    "nn = get_nn(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(start_time)\n",
    "nn.predict()\n",
    "print('inference time:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
