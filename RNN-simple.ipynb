{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys, os, gc, types\n",
    "import time\n",
    "from subprocess import check_output\n",
    "import tensorflow as tf\n",
    "\n",
    "from datareader_rnn import DataReader\n",
    "from tf_basemodel import TFBaseModel\n",
    "from tf_utils import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/junxie/Dropbox/JuanCode/Insight/project/data_mini/\n"
     ]
    }
   ],
   "source": [
    "root_paths = [\n",
    "    \"/Users/jiayou/Dropbox/JuanCode/Kaggle/Wikipedia/data2/\", # Mac\n",
    "    \"/Users/jiayou/Dropbox/Documents/JuanCode/Kaggle/Wikipedia/data2/\", # 1080\n",
    "    '/Users/junxie/Dropbox/JuanCode/Insight/project/data_mini/', # pro\n",
    "    '/mnt/WD Black/Dropbox/JuanCode/Insight/Project/data_mini/', # paperspace\n",
    "]\n",
    "root = None\n",
    "for p in root_paths:\n",
    "    if os.path.exists(p):\n",
    "        root = p\n",
    "        break\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WikiRNN(TFBaseModel):\n",
    "\n",
    "    def __init__(self, state_size, keep_prob=1, **kwargs):\n",
    "        self.state_size = state_size\n",
    "        self.keep_prob = keep_prob\n",
    "        super(type(self), self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        self.data = tf.placeholder(tf.float32, name='data')\n",
    "        self.given_days = tf.placeholder(tf.int32, name='given_days')\n",
    "        self.no_loss_days = tf.placeholder(tf.int32, name='no_loss_days')\n",
    "        self.days = tf.placeholder(tf.int32, name='days')\n",
    "        batch_size = tf.shape(self.data)[0]\n",
    "        \n",
    "#         batch_size = tf.Print(batch_size, [self.data], \"data\", summarize=1000)\n",
    "        \n",
    "        # Features\n",
    "        self.dayofweek = tf.placeholder(tf.int32, [None])\n",
    "        self.isweekday = tf.placeholder(tf.int32, [None])\n",
    "        self.month = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.domain = tf.placeholder(tf.int32, [None])\n",
    "        self.agent = tf.placeholder(tf.int32, [None])\n",
    "        self.access = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        dayofweek_oh = tf.one_hot(self.dayofweek, 7)\n",
    "        isweekday_oh = tf.one_hot(self.isweekday, 2)\n",
    "        month = tf.one_hot(self.month, 13)\n",
    "        \n",
    "        domain = tf.one_hot(self.domain, 9)\n",
    "        agent = tf.one_hot(self.agent, 2)\n",
    "        access = tf.one_hot(self.access, 3)\n",
    "        \n",
    "        date_features = tf.concat(\n",
    "            [\n",
    "                dayofweek_oh,\n",
    "                isweekday_oh,\n",
    "                month,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        date_features = tf.tile(tf.expand_dims(date_features, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        page_features = tf.concat(\n",
    "            [\n",
    "                domain,\n",
    "                agent,\n",
    "                access,\n",
    "            ], \n",
    "            axis=1,\n",
    "        )\n",
    "        page_features = tf.tile(tf.expand_dims(page_features, 1), [1, self.days, 1])\n",
    "        \n",
    "        features = tf.concat([date_features, page_features], axis=2)\n",
    "        \n",
    "        cells = []\n",
    "        for i in range(len(self.state_size)):\n",
    "            c = tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.LSTMCell(\n",
    "                    self.state_size[i],\n",
    "                ),\n",
    "                output_keep_prob=self.keep_prob,\n",
    "            )\n",
    "            if i != 0:\n",
    "                c = tf.nn.rnn_cell.ResidualWrapper(c)\n",
    "            cells.append(c)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "        \n",
    "        # ([batch_size, state_size])\n",
    "        state = cell.zero_state(tf.shape(self.data)[0], dtype=tf.float32)\n",
    "        # [batch_size, 1]\n",
    "        last_output = tf.zeros([tf.shape(self.data)[0], 1], dtype=tf.float32)\n",
    "        \n",
    "        loss = tf.constant(0, dtype=tf.float32)\n",
    "        step = tf.constant(0, dtype=tf.int32)\n",
    "        output_ta = tf.TensorArray(size=self.days, dtype=tf.float32)\n",
    "        \n",
    "        def cond(last_output, state, loss, step, output_ta):\n",
    "            return step < self.days\n",
    "        \n",
    "        def body(last_output, state, loss, step, output_ta):\n",
    "            inp = tf.concat(\n",
    "                [\n",
    "                    last_output,\n",
    "                    features[:, step, :],\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "#             inp = tf.cond(\n",
    "#                 step < 10,\n",
    "#                 lambda: tf.Print(inp, [step, inp], \"input\", summarize=200*47),\n",
    "#                 lambda: inp,\n",
    "#             )\n",
    "            \n",
    "            output, state = cell(inp, state)\n",
    "            output = tf.layers.dense(\n",
    "                output,\n",
    "                1,\n",
    "                name='dense-top'\n",
    "            )\n",
    "            output_ta = output_ta.write(step, tf.transpose(output))\n",
    "            \n",
    "            last_output = tf.cond(\n",
    "                step < self.given_days,\n",
    "                lambda: tf.expand_dims(self.data[:,step], 1),\n",
    "                lambda: output,\n",
    "            )\n",
    "            last_output.set_shape([None, 1])\n",
    "            \n",
    "#             true = tf.maximum(1e-8, self.data[:,step])            \n",
    "            true = tf.expand_dims(self.data[:,step], 1)\n",
    "#             true = tf.Print(true, [true], 'true.shape', summarize=1000)\n",
    "#             output = tf.Print(output, [output], 'output.shape', summarize=1000)\n",
    "            loss = tf.cond(\n",
    "                step >= self.no_loss_days,\n",
    "#                 lambda: loss + tf.reduce_mean(2 * tf.abs(true - output) / tf.maximum(1e-6, true + output)),\n",
    "                lambda: loss + tf.reduce_mean(tf.abs(true - output)),\n",
    "                lambda: loss\n",
    "            )\n",
    "            loss.set_shape([])\n",
    "            \n",
    "            return (last_output, state, loss, step + 1, output_ta)\n",
    "        \n",
    "        _, self.final_state, loss, _, output_ta = tf.while_loop(\n",
    "            cond=cond,\n",
    "            body=body,\n",
    "            loop_vars=(last_output, state, loss, step, output_ta)\n",
    "        )\n",
    "        \n",
    "        self.preds = tf.transpose(output_ta.concat())\n",
    "        self.prediction_tensors = {\n",
    "            'preds': self.preds\n",
    "        }\n",
    "        \n",
    "        loss = loss / tf.cast(self.days - self.no_loss_days, tf.float32)\n",
    "#         loss = tf.Print(loss, [loss, self.data[:, -1], self.preds[:, -1]], \"Loss = \")\n",
    "        return loss\n",
    "\n",
    "    def predict(self, batch_size=1000, num_batches=None):\n",
    "        preds = []\n",
    "        states = []\n",
    "        test_generator = self.reader.test_batch_generator(batch_size)\n",
    "        for i, test_batch_df in enumerate(test_generator):\n",
    "            test_feed_dict = {\n",
    "                getattr(self, placeholder_name, None): data\n",
    "                for placeholder_name, data in test_batch_df.items() if hasattr(self, placeholder_name)\n",
    "            }\n",
    "\n",
    "            batch_preds, batch_states = self.session.run(\n",
    "                fetches=[self.preds, self.final_state],\n",
    "                feed_dict=test_feed_dict\n",
    "            )\n",
    "            \n",
    "            sc_std = test_batch_df['sc_std']\n",
    "            sc_mean = test_batch_df['sc_mean']\n",
    "            batch_preds = self.reader.inverse_transform(batch_preds, sc_std=sc_std, sc_mean=sc_mean)\n",
    "            \n",
    "            preds.append(batch_preds)\n",
    "            states.append(batch_states)\n",
    "            print('batch {} processed'.format(i))\n",
    "\n",
    "        return (np.concatenate(preds, axis=0), states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_nn(reader):\n",
    "    # 2000 steps make an epoch for all data (200 steps for mini data)\n",
    "    return WikiRNN(\n",
    "        name='nn_v1',\n",
    "        reader=reader,\n",
    "        work_dir='./tf-data',\n",
    "        optimizer='adam',\n",
    "        learning_rate=.001,\n",
    "        batch_size=128,\n",
    "        num_validation_batches=1,\n",
    "        num_training_steps=1000000,\n",
    "        early_stopping_steps=2000,\n",
    "        num_restarts=3,\n",
    "        warm_start_init_step=0,\n",
    "        regularization_constant=0.0,\n",
    "        enable_parameter_averaging=False,\n",
    "        min_steps_to_checkpoint=2000,\n",
    "        loss_averaging_window=2000,\n",
    "        log_interval=100,\n",
    "\n",
    "        state_size=[300, 300],\n",
    "        keep_prob=1\n",
    "    )\n",
    "\n",
    "reader = DataReader(\n",
    "    data, fpage=fpage, fdate=fdate, \n",
    "    min_train_days=100, \n",
    "    max_train_days=700, \n",
    "    train_predict_days=72, \n",
    "    train_loss_days=72,\n",
    "    val_days=72,\n",
    "    \n",
    "    predict_days=64,\n",
    "    predict_warmup_days=803,\n",
    "    seed=923\n",
    ")\n",
    "\n",
    "nn = get_nn(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def smape(true, pred, sc_std, sc_mean, reader):\n",
    "    t = (reader.inverse_transform(true, sc_std=sc_std, sc_mean=sc_mean))\n",
    "    p = (reader.inverse_transform(pred, sc_std=sc_std, sc_mean=sc_mean))\n",
    "    p = np.round(np.maximum(0, p))\n",
    "    return (np.abs(t - p) * 200 / np.maximum(1e-10, (t + p))).reshape((-1))\n",
    "    \n",
    "\n",
    "val_gen = reader.val_batch_generator(2000)\n",
    "smapes = []\n",
    "# start_time = time.time()\n",
    "\n",
    "for step in nn.fit(yield_interval=20):\n",
    "#     print('Training step {} started, it cost {} secs so far'.format(step, time.time()-start_time))\n",
    "    val_batch_df = next(val_gen)\n",
    "    feed_dict = {\n",
    "        getattr(nn, placeholder_name, None): data\n",
    "        for placeholder_name, data in val_batch_df.items() if hasattr(nn, placeholder_name)\n",
    "    }\n",
    "\n",
    "    loss_l1, preds = nn.session.run(\n",
    "        fetches=[nn.loss, nn.preds],\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    loss_smape = smape(\n",
    "        val_batch_df['data'][:, -reader.train_loss_days:], \n",
    "        preds[:, -reader.train_loss_days:], \n",
    "        sc_std=val_batch_df['sc_std'],\n",
    "        sc_mean=val_batch_df['sc_mean'],\n",
    "        reader=reader\n",
    "    ).mean()\n",
    "    smapes.append(loss_smape)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.title('smape')\n",
    "    plt.plot(smapes, '*-')\n",
    "    plt.show()\n",
    "    \n",
    "    print('val: smape = {}, l1 = {}'.format(loss_smape, loss_l1))\n",
    "    \n",
    "    for i in range(1):\n",
    "        idx = np.random.randint(0, 1000)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        true = val_batch_df['data'][idx, :]\n",
    "        plt.plot(true, 'g--')\n",
    "        pred = preds[idx, :]\n",
    "        plt.plot(pred, 'k.')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = DataReader(\n",
    "    root,\n",
    "    min_train_days=100, \n",
    "    max_train_days=700, \n",
    "    train_predict_days=72, \n",
    "    train_loss_days=72,\n",
    "    val_days=72,\n",
    "    \n",
    "    predict_days=64,\n",
    "    predict_warmup_days=803,\n",
    "    seed=923\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = get_nn(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds, states = nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(root, 'pred.{}.npy'.format(nn.name)), np.round(preds[:,-62:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = DataReader(\n",
    "    root, \n",
    "    min_train_days=100, \n",
    "    max_train_days=700, \n",
    "    train_predict_days=62, \n",
    "    train_loss_days=62,\n",
    "    val_days=62,\n",
    "    \n",
    "    predict_days=0,\n",
    "    predict_warmup_days=793 - 62,\n",
    "    seed=923\n",
    ")\n",
    "\n",
    "nn = get_nn(reader)\n",
    "\n",
    "nn.restore()\n",
    "preds, states = nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_list = []\n",
    "for i in range(len(states)):\n",
    "    concat_list.append(\n",
    "        np.concatenate([states[i][layer].c for layer in range(len(states[i]))], axis=1)\n",
    "    )\n",
    "page_ft = np.concatenate(concat_list, axis=0)\n",
    "page_ft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join(root, 'states.{}.npy'.format(nn.name)), page_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
