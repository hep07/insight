{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Future Values of Multiple Time Series\n",
    "my Insight AI project\n",
    "\n",
    "## The motivation\n",
    "Data in the form of time-dependent sequential observations emerge in many key real-world problems, including areas such as biological data, financial markets, demand and supply forecasting, signal collected in IoT and wearable devices, to audio and video processing. The tranditional methods (such as ARIMA) that are based on statistical features (Exponential Moving Average) can do pretty well in single time series prediction, but for multiple time series task, it can not generate predictions based on related time series that have similar pattern. Future values may depends on the previous value, it may also depends on other external time series that has similarity. \n",
    "\n",
    "Recurrent Neural networks, and other newer architectures, such as Wavenet and AttentionNet, have showed their success in audio processing and lauguage translation. It's an intersting topic to see how those deep learning networks perform on multiple time series data, given the fact that they are able to learn the similarities across different time series.\n",
    "\n",
    "## The task\n",
    "The objective of this project is to predict multiple sequences of future values, given the sequences of historical time series data.\n",
    "\n",
    "The data I explored with are web traffic data of Wikipedia, which is an accessible large dataset. (https://www.kaggle.com/c/web-traffic-time-series-forecasting) The training dataset consists of approximately 145k time series. Each of these time series represents a number of daily views of a different Wikipedia article, starting from July 1st, 2015 up until September 10th, 2017. The goal is to forecast the daily views between September 13th, 2017 and November 13th, 2017 for each article in the test dataset. The name of the article as well as the type of traffic (all, mobile, desktop, spider) is given for each article. The y-axis is log transformed.\n",
    "<p align=\"center\">\n",
    "  <img src=\"figures/wiki_data1.png\">\n",
    "</p>\n",
    "\n",
    "The evaluation metric is symmetric mean absolute percentage error (SMAPE).\n",
    "\n",
    "## The approach\n",
    "Several models are implemented and benchmarked on this dataset, includes a gradient boosting decision tree, and deep learning models inspired by [LSTM RNN (recurrent neural network)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Wavenet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), and [AttentionNet](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html).\n",
    "\n",
    "## Requirements\n",
    "11 GB GPU (recommended), Python 3\n",
    "\n",
    "Python packages:\n",
    "- numpy==1.13.3\n",
    "- pandas==0.21.0\n",
    "- matplotlib==2.1.0\n",
    "- scikit-learn==0.19.1\n",
    "- lightgbm==2.0.10\n",
    "- tensorflow==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
